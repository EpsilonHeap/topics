{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### License\n",
    "Licensed under the BSD 3-Clause License (the \"License\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BSD 3-Clause License\n",
    "\n",
    "Copyright (c) 2017, Pytorch contributors\n",
    "All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "* Redistributions of source code must retain the above copyright notice, this\n",
    "  list of conditions and the following disclaimer.\n",
    "\n",
    "* Redistributions in binary form must reproduce the above copyright notice,\n",
    "  this list of conditions and the following disclaimer in the documentation\n",
    "  and/or other materials provided with the distribution.\n",
    "\n",
    "* Neither the name of the copyright holder nor the names of its\n",
    "  contributors may be used to endorse or promote products derived from\n",
    "  this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper ['Fast Sparse Regression and Classification' (2008)](http://statweb.stanford.edu/~jhf/ftp/GPSpaper.pdf), [Jerome Friedman](https://statweb.stanford.edu/~jhf/) introduces the generalized path seeking (GPS) algorithm to directly construct, sequentially, a path in parameter space that approximates that for a given penalty $P(a)$ on the coffefficents $a$ of an associated regression model. This Jupyter notebook is an implementation of GPS that takes advantage of the 'autograd' facility provided by many machine learning frameworks for calculating gradients. [PyTorch (1.0.1)](https://pytorch.org/) will be used in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple model and loss function to demonstrate how useful autograd will be for implementing GPS. $a$ is a vector of coefficients of the regression model and is exposed at the toplevel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 9\n",
    "a = torch.zeros(N,requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "def Fmodel(a, x):\n",
    "    \"\"\" a is the coefficient vector of a linear regression model\n",
    "        a[0] is the constant term\n",
    "        x is a matrix of data where each row is an observation \"\"\"\n",
    "    return (x @ a[1:]) + a[0]\n",
    "\n",
    "def Mse(t1,t2):\n",
    "    \"\"\" mean square error \"\"\"\n",
    "    diff = t1-t2\n",
    "    return torch.sum(diff*diff)/diff.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a concrete example, we will use data from [Stamey et al (1989)](https://www.ncbi.nlm.nih.gov/pubmed/2468795) that can be accessed online at [Robert Tibshirani's](https://statweb.stanford.edu/~tibs) website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45',\n",
      "       'lpsa', 'train'],\n",
      "      dtype='object')\n",
      "tensor([[-0.5798,  2.7695, 50.0000, -1.3863,  0.0000, -1.3863,  6.0000,  0.0000],\n",
      "        [-0.9943,  3.3196, 58.0000, -1.3863,  0.0000, -1.3863,  6.0000,  0.0000],\n",
      "        [-0.5108,  2.6912, 74.0000, -1.3863,  0.0000, -1.3863,  7.0000, 20.0000],\n",
      "        [-1.2040,  3.2828, 58.0000, -1.3863,  0.0000, -1.3863,  6.0000,  0.0000],\n",
      "        [ 0.7514,  3.4324, 62.0000, -1.3863,  0.0000, -1.3863,  6.0000,  0.0000],\n",
      "        [-1.0498,  3.2288, 50.0000, -1.3863,  0.0000, -1.3863,  6.0000,  0.0000],\n",
      "        [ 0.7372,  3.4735, 64.0000,  0.6152,  0.0000, -1.3863,  6.0000,  0.0000],\n",
      "        [ 0.6931,  3.5395, 58.0000,  1.5369,  0.0000, -1.3863,  6.0000,  0.0000],\n",
      "        [-0.7765,  3.5395, 47.0000, -1.3863,  0.0000, -1.3863,  6.0000,  0.0000],\n",
      "        [ 0.2231,  3.2445, 63.0000, -1.3863,  0.0000, -1.3863,  6.0000,  0.0000]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pdata = pd.read_csv(\"http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data\",\n",
    "                    index_col=0,sep='\\t')\n",
    "# lpsa values are the y[i]'s that we want to predict\n",
    "lpsa = torch.from_numpy(pdata.lpsa.values)\n",
    "print(pdata.columns)\n",
    "xvals = torch.from_numpy(pdata[['lcavol','lweight','age','lbph','svi','lcp','gleason','pgg45']].values)\n",
    "print(xvals[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = Fmodel(a, xvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4308, -0.1625, -0.1625, -0.1625,  0.3716,  0.7655,  0.7655,  0.8544,\n",
       "         1.0473,  1.0473], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpsa[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.4611, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = Mse(preds,lpsa)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch keeps track of the mathematical operations performed on $a$ and will automatically calculate the gradient using reverse-mode differentiation that is often used for back-propagation in neural nets. In PyTorch, invoking the backword() method on a scalar tensor will automatically calculate the gradient. Note that gradients cumulate and will need to be zeroed out where appropriate for the calculation at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "tensor([  -4.9568,   -8.6696,  -18.4120, -319.4542,   -1.0935,   -1.6087,\n",
      "          -0.8643,  -34.0798, -148.0683], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(a)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to updating the coefficients and minimize the loss is to iteratively move along the direction of the gradient. We will use this idea later to address a minor issue with the GPS algorithm as stated in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    preds = Fmodel(a, xvals)\n",
    "    loss = Mse(preds, lpsa)\n",
    "    loss.backward()\n",
    "    # no_grad() context because we do not want to calcualte but update a\n",
    "    with torch.no_grad():\n",
    "        a -= a.grad * 1e-5\n",
    "        a.grad.zero_()\n",
    "a_ref = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8563, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4125, 1.5940, 2.2870, 1.5660, 1.9234, 1.3732, 2.0223, 1.8837, 1.3424,\n",
      "        1.8751, 1.9471, 1.7544, 2.4407, 2.2096, 2.0001, 2.1117, 2.4514, 2.3558,\n",
      "        1.2002, 2.1494, 1.8940, 2.5385, 1.6929, 2.9630, 2.1405, 2.1942, 2.7896,\n",
      "        2.2424, 3.0851, 2.4215, 2.0914, 2.0184, 2.2699, 1.6132, 1.8863, 2.2407,\n",
      "        2.7582, 2.0804, 3.0699, 1.9769, 2.7702, 2.3134, 2.0606, 2.2945, 2.5000,\n",
      "        2.2440, 4.1743, 2.7475, 1.5530, 2.3040, 2.7018, 2.2052, 2.8866, 3.0069,\n",
      "        2.2331, 2.3985, 1.7394, 1.6258, 2.4212, 2.4656, 2.3084, 2.9030, 3.7387,\n",
      "        3.2277, 2.1504, 2.3739, 3.2891, 2.6829, 1.9986, 2.5101, 2.9001, 2.7291,\n",
      "        2.5677, 3.2882, 2.9128, 3.2606, 3.2614, 2.8288, 3.4287, 2.9003, 2.7113,\n",
      "        2.9801, 3.1047, 3.3660, 2.5567, 3.2542, 2.0618, 2.5395, 3.2797, 3.4611,\n",
      "        2.4237, 2.4141, 3.2438, 2.5904, 2.3703, 3.5405, 3.0644],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4308, -0.1625, -0.1625, -0.1625,  0.3716,  0.7655,  0.7655,  0.8544,\n",
      "         1.0473,  1.0473,  1.2669,  1.2669,  1.2669,  1.3481,  1.3987,  1.4469,\n",
      "         1.4702,  1.4929,  1.5581,  1.5994,  1.6390,  1.6582,  1.6956,  1.7138,\n",
      "         1.7317,  1.7664,  1.8001,  1.8165,  1.8485,  1.8946,  1.9242,  2.0082,\n",
      "         2.0082,  2.0215,  2.0477,  2.0857,  2.1576,  2.1917,  2.2138,  2.2773,\n",
      "         2.2976,  2.3076,  2.3273,  2.3749,  2.5217,  2.5533,  2.5688,  2.5688,\n",
      "         2.5915,  2.5915,  2.6568,  2.6776,  2.6844,  2.6912,  2.7047,  2.7180,\n",
      "         2.7881,  2.7942,  2.8064,  2.8124,  2.8420,  2.8536,  2.8536,  2.8820,\n",
      "         2.8820,  2.8876,  2.9205,  2.9627,  2.9627,  2.9730,  3.0131,  3.0374,\n",
      "         3.0564,  3.0750,  3.2753,  3.3375,  3.3928,  3.4356,  3.4579,  3.5130,\n",
      "         3.5160,  3.5308,  3.5653,  3.5709,  3.5877,  3.6310,  3.6801,  3.7124,\n",
      "         3.9843,  3.9936,  4.0298,  4.1296,  4.3851,  4.6844,  5.1431,  5.4775,\n",
      "         5.5829], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(lpsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central to the implementation of GPS are equations (24), (25), and (26) defined on page 6 of the article - reproduced below. Equations (24) and (25) captures the gradient of empirical 'risk' $\\hat{R}(a)$ and the penalty $P(a)$ used to regularized the model, respectively. Both are directional vectors in the parameter space of a regression problem. While (24) is directly depedent on the model and data, (25) has a more 'universal' nature in that they are gradients of a penalty with respect to parameters -- which typically have a form applicable across models. Note that $\\nu$ is a parameterization of the steps size in parameter space, and the 'hat' symbol e.g. ($\\hat{R}$ and $\\hat{a}$) signifies empirical quantities that are explicit dependent on the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "g_{j}(\\nu) & = & - & \n",
    "\\left[\\frac{\\partial \\hat{R}(a)}{\\partial a_{j}}\\right]_{a=\\hat{a}(\\nu)} & \\hspace{1in} (24) \\\\\n",
    "p_{j}(\\nu) & = & & \n",
    "\\left[\\frac{\\partial P(a)}{\\partial \\left| a_{j} \\right|} \\right]_{a=\\hat{a}(\\nu)} & \\hspace{1in} (25) \\\\\n",
    "\\lambda_{j}(\\nu) & = & & \n",
    "\\frac{g_{j}(\\nu)}{p_{j}(\\nu)} & \\hspace{1in} (26)\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before putting together the whole algorithm, each piece will be demonstrated separately. Without going into details at this point (see Section 2.3 of the paper), $a$ will be reset to zero and slightly pushed along the negative gradient as a starting point for this demonstration. The difficulty with zero is not surprising when dealing with gradients of absolute values evaluate at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 9\n",
    "a = torch.zeros(N,requires_grad=True, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Mse(Fmodel(a, xvals), lpsa)\n",
    "loss.backward()\n",
    "with torch.no_grad():\n",
    "    a -= a.grad * 1e-5\n",
    "    a.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Compute \\{\\lambda_{j}(\\nu)\\}^{n}_{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.9568e-05, 8.6696e-05, 1.8412e-04, 3.1945e-03, 1.0935e-05, 1.6087e-05,\n",
      "        8.6427e-06, 3.4080e-04, 1.4807e-03], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "tensor(6.2674, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(1.2558e-05, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor([9.9135e-05, 1.7339e-04, 3.6824e-04, 6.3891e-03, 2.1870e-05, 3.2174e-05,\n",
      "        1.7285e-05, 6.8160e-04, 2.9614e-03], dtype=torch.float64)\n",
      "tensor([  4.4702,   7.9575,  16.6355, 287.8595,   1.0111,   1.4853,   0.8695,\n",
      "         30.7401, 133.4941], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "R = Mse(Fmodel(a, xvals), lpsa);R.backward(); g = -a.grad; a.grad.zero_() #(24)\n",
    "print(R)\n",
    "#P = abs(a).pow(1/2).sum(); P.backward(); p = abs(a.grad); a.grad.zero_() #(25)\n",
    "P = abs(a).pow(2).sum(); P.backward(); p = abs(a.grad); a.grad.zero_() #(25)\n",
    "print(P)\n",
    "l = g/p #(26)\n",
    "print(p)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ S = \\{ j \\, | \\, \\lambda_{j}(\\nu) * \\hat{a}_{j}(\\nu) < 0 \\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([45092.3164, 45893.0302, 45175.6898, 45054.8953, 46234.0950, 46164.1531,\n",
      "        50303.5029, 45100.0898, 45078.5758], dtype=torch.float64)\n",
      "tensor([4.9568e-05, 8.6696e-05, 1.8412e-04, 3.1945e-03, 1.0935e-05, 1.6087e-05,\n",
      "        8.6427e-06, 3.4080e-04, 1.4807e-03], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# use element wise multiplication and less than 0 predicate\n",
    "# to find elements with corresponding opposite sign\n",
    "S = l*a < 0\n",
    "print(l)\n",
    "print(a)\n",
    "print (S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "if \\; (S = empty) \\hspace{5pt} & j^{*} & = arg\\,max_{j} & | \\lambda_{j}(\\nu) | \\\\\n",
    "else \\hspace{5pt} & j^{*} & = arg\\,max_{j \\in S} & | \\lambda_{j}(\\nu) | \n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(50303.5029, dtype=torch.float64) tensor(6)\n"
     ]
    }
   ],
   "source": [
    "# need to maintain idx location, i.e. need to keep tensor shape the same throughout\n",
    "# torch.max() finds max element and respective index\n",
    "if S.sum() > 0: # check for elements in set\n",
    "    # non empty case (order different than stated algorithm)\n",
    "    # S.double() for matching element types needed by PyTorch\n",
    "    # element wise mult to zero out elements not meeting predicate condition\n",
    "    # done this way to make sure idx refer to corresponding element\n",
    "    val,idx = torch.max(S.double() * l.abs(), 0)\n",
    "else:\n",
    "    # empty case\n",
    "    val,idx = torch.max(l.abs(), 0)\n",
    "print(val,idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{a}_{j^{*}}(\\nu + \\Delta \\nu) = \\hat{a}_{j^{*}}(\\nu) + \\Delta \\nu * sign(\\lambda_{j^{*}}(\\nu)) \\\\\n",
    " \\{ \\hat{a}_{j}(\\nu + \\Delta \\nu) = \\hat{a}_{j}(\\nu) \\}_{j \\ne j^{*}} $$\n",
    "$$ \\nu \\leftarrow \\nu + \\Delta \\nu $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6) tensor(8.6427e-06, dtype=torch.float64, grad_fn=<SelectBackward>) tensor(50303.5029, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# update single component, a[idx], with new value\n",
    "# here are the values at play for this iteration\n",
    "print(idx, a[idx], l[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a[idx] compoment is updated with a new value, the rest remain unchanged. The code would then be something along the line of:\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    a[idx] += del_nu * torch.sign(l[idx])\n",
    "    a.grad.zero_()\n",
    "```\n",
    "Choosing $\\Delta \\nu$ is an implementation decision.\n",
    "Note that $\\Delta \\nu$ is an implied change in the parameterized path of $a(\\nu)$ that would bring about a change of $\\Delta a$. Since $sign(\\lambda_{j^{*}}(\\nu))$ contributes only the sign of the change, $\\Delta \\nu$ is effectively the magnitude of $\\Delta a$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 9.4 of the paper suggests one approach to setting the step size: chose $\\Delta \\nu$ to reduce the empirical risk $\\hat{R}(\\hat{a})$ by a fixed fraction $\\epsilon$.\n",
    "\n",
    "$$ \\frac{\\left [ \\hat{R}(\\hat{a}(\\nu)) - \\hat{R}(\\hat{a}(\\nu + \\Delta \\nu)) \\right]}\n",
    "{\\hat{R}(\\hat{a}(\\nu))} = \\epsilon $$\n",
    "\n",
    "The algorithm updates one component $a_{j^{*}}$ at a time. An approximation for $\\epsilon$ is then\n",
    "\n",
    "$$ \\left | \\frac{g_{j^{*}}(\\nu) * \\Delta a_{j^{*}}}{\\hat{R}(a(\\nu))_{a=\\hat{a}(\\nu)}} \\right | \\approx \\epsilon $$\n",
    "\n",
    "With a choice of $\\epsilon$ = 0.01,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0084, dtype=torch.float64, grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    del_nu = 0.01 * (R / g[idx]).abs()\n",
    "    a[idx] += del_nu * torch.sign(l[idx])\n",
    "    R_post = Mse(Fmodel(a, xvals), lpsa)\n",
    "    a.grad.zero_()\n",
    "    \n",
    "# should be 'close' to 0.01\n",
    "print(1-(R_post/R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPS algorithm with all the pieces composed together.\n",
    "\n",
    "Line numbering are diffent from the paper because the breakdown above was composed as units that better match this description.\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "1 & Initialize: \\nu = 0; \\{\\hat{a}_{j}(0) = 0\\}_{1}^{n} \\\\\n",
    "2 & Loop \\; \\{ \\\\\n",
    "3 & \\hspace{10pt} Compute \\{\\lambda_{j}(\\nu)\\}^{n}_{1} \\\\\n",
    "4 & \\hspace{10pt} S = \\{ j \\, | \\, \\lambda_{j}(\\nu) * \\hat{a}_{j}(\\nu) < 0 \\} \\\\\n",
    "5 & \\hspace{10pt} \\begin{eqnarray}\n",
    "if \\; (S = empty) \\hspace{5pt} & j^{*} & = arg\\,max_{j} & | \\lambda_{j}(\\nu) | \\\\\n",
    "else \\hspace{10pt} & j^{*} & = arg\\,max_{j \\in S} & | \\lambda_{j}(\\nu) |\n",
    "\\end{eqnarray} \\\\\n",
    "6 & \\hspace{10pt} \\hat{a}_{j^{*}}(\\nu + \\Delta \\nu) = \\hat{a}_{j^{*}}(\\nu) + \\Delta \\nu * sign(\\lambda_{j^{*}}(\\nu)); \\{ \\hat{a}_{j}(\\nu + \\Delta \\nu) = \\hat{a}_{j}(\\nu) \\}_{j \\ne j^{*}} \\\\\n",
    "7 & \\hspace{10pt} \\nu \\leftarrow \\nu + \\Delta \\nu \\\\\n",
    "8 & \\} \\; Until \\; \\lambda(\\nu) = 0\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(6.2674, dtype=torch.float64) tensor(0.0054, dtype=torch.float64)\n",
      "100 tensor(2.3031, dtype=torch.float64) tensor(0.0243, dtype=torch.float64)\n",
      "200 tensor(1.5662, dtype=torch.float64) tensor(0.0378, dtype=torch.float64)\n",
      "300 tensor(1.0992, dtype=torch.float64) tensor(0.0550, dtype=torch.float64)\n",
      "400 tensor(3.1681, dtype=torch.float64) tensor(0.0332, dtype=torch.float64)\n",
      "500 tensor(1.1694, dtype=torch.float64) tensor(0.0514, dtype=torch.float64)\n",
      "600 tensor(1.7747, dtype=torch.float64) tensor(0.0500, dtype=torch.float64)\n",
      "700 tensor(4.8778, dtype=torch.float64) tensor(0.0326, dtype=torch.float64)\n",
      "800 tensor(1.7930, dtype=torch.float64) tensor(0.0498, dtype=torch.float64)\n",
      "900 tensor(1.1154, dtype=torch.float64) tensor(0.0672, dtype=torch.float64)\n",
      "1000 tensor(6.0591, dtype=torch.float64) tensor(0.0403, dtype=torch.float64)\n",
      "1100 tensor(2.2260, dtype=torch.float64) tensor(0.0536, dtype=torch.float64)\n",
      "1200 tensor(3.6075, dtype=torch.float64) tensor(0.0528, dtype=torch.float64)\n",
      "1300 tensor(1.3280, dtype=torch.float64) tensor(0.0696, dtype=torch.float64)\n",
      "1400 tensor(1.0714, dtype=torch.float64) tensor(0.0764, dtype=torch.float64)\n",
      "1500 tensor(16.5698, dtype=torch.float64) tensor(0.1687, dtype=torch.float64)\n",
      "1600 tensor(6.0823, dtype=torch.float64) tensor(0.1231, dtype=torch.float64)\n",
      "1700 tensor(2.2344, dtype=torch.float64) tensor(0.0921, dtype=torch.float64)\n",
      "1800 tensor(1.7430, dtype=torch.float64) tensor(0.0771, dtype=torch.float64)\n",
      "1900 tensor(1.0678, dtype=torch.float64) tensor(0.0900, dtype=torch.float64)\n",
      "2000 tensor(1.4499, dtype=torch.float64) tensor(0.0824, dtype=torch.float64)\n",
      "2100 tensor(1.1354, dtype=torch.float64) tensor(0.0910, dtype=torch.float64)\n",
      "2200 tensor(3.5325, dtype=torch.float64) tensor(0.0729, dtype=torch.float64)\n",
      "2300 tensor(1.3004, dtype=torch.float64) tensor(0.0895, dtype=torch.float64)\n",
      "2400 tensor(1.6450, dtype=torch.float64) tensor(0.0910, dtype=torch.float64)\n",
      "2500 tensor(1.0967, dtype=torch.float64) tensor(0.1026, dtype=torch.float64)\n",
      "2600 tensor(1.0615, dtype=torch.float64) tensor(0.1098, dtype=torch.float64)\n",
      "2700 tensor(13.6321, dtype=torch.float64) tensor(0.1062, dtype=torch.float64)\n",
      "2800 tensor(5.0043, dtype=torch.float64) tensor(0.0831, dtype=torch.float64)\n",
      "2900 tensor(1.8390, dtype=torch.float64) tensor(0.1000, dtype=torch.float64)\n",
      "3000 tensor(1.5585, dtype=torch.float64) tensor(0.1027, dtype=torch.float64)\n",
      "3100 tensor(7.5384, dtype=torch.float64) tensor(0.0985, dtype=torch.float64)\n",
      "3200 tensor(2.7684, dtype=torch.float64) tensor(0.1009, dtype=torch.float64)\n",
      "3300 tensor(1.0245, dtype=torch.float64) tensor(0.1194, dtype=torch.float64)\n",
      "3400 tensor(4.3901, dtype=torch.float64) tensor(0.0930, dtype=torch.float64)\n",
      "3500 tensor(1.6139, dtype=torch.float64) tensor(0.1094, dtype=torch.float64)\n",
      "3600 tensor(1.1801, dtype=torch.float64) tensor(0.1151, dtype=torch.float64)\n",
      "3700 tensor(1.0393, dtype=torch.float64) tensor(0.1245, dtype=torch.float64)\n",
      "3800 tensor(1.4387, dtype=torch.float64) tensor(0.1188, dtype=torch.float64)\n",
      "3900 tensor(5.7698, dtype=torch.float64) tensor(0.1668, dtype=torch.float64)\n",
      "4000 tensor(2.1196, dtype=torch.float64) tensor(0.1367, dtype=torch.float64)\n",
      "4100 tensor(1.1785, dtype=torch.float64) tensor(0.1432, dtype=torch.float64)\n",
      "4200 tensor(1.5721, dtype=torch.float64) tensor(0.1394, dtype=torch.float64)\n",
      "4300 tensor(2.1455, dtype=torch.float64) tensor(0.1611, dtype=torch.float64)\n",
      "4400 tensor(0.9586, dtype=torch.float64) tensor(0.1599, dtype=torch.float64)\n",
      "4500 tensor(1.9262, dtype=torch.float64) tensor(0.1579, dtype=torch.float64)\n",
      "4600 tensor(1.0623, dtype=torch.float64) tensor(0.1501, dtype=torch.float64)\n",
      "4700 tensor(1.2451, dtype=torch.float64) tensor(0.1461, dtype=torch.float64)\n",
      "4800 tensor(2.7231, dtype=torch.float64) tensor(0.1673, dtype=torch.float64)\n",
      "4900 tensor(1.0067, dtype=torch.float64) tensor(0.1529, dtype=torch.float64)\n",
      "5000 tensor(1.0735, dtype=torch.float64) tensor(0.1537, dtype=torch.float64)\n",
      "5100 tensor(1.4553, dtype=torch.float64) tensor(0.1508, dtype=torch.float64)\n",
      "5200 tensor(1.7280, dtype=torch.float64) tensor(0.1604, dtype=torch.float64)\n",
      "5300 tensor(0.9637, dtype=torch.float64) tensor(0.1717, dtype=torch.float64)\n",
      "5400 tensor(4.3704, dtype=torch.float64) tensor(0.1973, dtype=torch.float64)\n",
      "5500 tensor(1.6063, dtype=torch.float64) tensor(0.1695, dtype=torch.float64)\n",
      "5600 tensor(1.1161, dtype=torch.float64) tensor(0.1666, dtype=torch.float64)\n",
      "5700 tensor(3.1548, dtype=torch.float64) tensor(0.1584, dtype=torch.float64)\n",
      "5800 tensor(1.1613, dtype=torch.float64) tensor(0.1741, dtype=torch.float64)\n",
      "5900 tensor(0.9672, dtype=torch.float64) tensor(0.1847, dtype=torch.float64)\n",
      "6000 tensor(7.7446, dtype=torch.float64) tensor(0.1611, dtype=torch.float64)\n",
      "6100 tensor(2.8438, dtype=torch.float64) tensor(0.1601, dtype=torch.float64)\n",
      "6200 tensor(1.0482, dtype=torch.float64) tensor(0.1762, dtype=torch.float64)\n",
      "6300 tensor(19.8386, dtype=torch.float64) tensor(0.1881, dtype=torch.float64)\n",
      "6400 tensor(7.2816, dtype=torch.float64) tensor(0.1597, dtype=torch.float64)\n",
      "6500 tensor(2.6740, dtype=torch.float64) tensor(0.1610, dtype=torch.float64)\n",
      "6600 tensor(0.9871, dtype=torch.float64) tensor(0.1778, dtype=torch.float64)\n",
      "6700 tensor(1.0193, dtype=torch.float64) tensor(0.1921, dtype=torch.float64)\n",
      "6800 tensor(0.9500, dtype=torch.float64) tensor(0.2004, dtype=torch.float64)\n",
      "6900 tensor(11.0768, dtype=torch.float64) tensor(0.2570, dtype=torch.float64)\n",
      "7000 tensor(4.0664, dtype=torch.float64) tensor(0.2190, dtype=torch.float64)\n",
      "7100 tensor(1.4948, dtype=torch.float64) tensor(0.1918, dtype=torch.float64)\n",
      "7200 tensor(1.1108, dtype=torch.float64) tensor(0.1895, dtype=torch.float64)\n",
      "7300 tensor(1.2475, dtype=torch.float64) tensor(0.1957, dtype=torch.float64)\n",
      "7400 tensor(35.0559, dtype=torch.float64) tensor(0.2354, dtype=torch.float64)\n",
      "7500 tensor(12.8655, dtype=torch.float64) tensor(0.1983, dtype=torch.float64)\n",
      "7600 tensor(4.7228, dtype=torch.float64) tensor(0.1749, dtype=torch.float64)\n",
      "7700 tensor(1.7353, dtype=torch.float64) tensor(0.1907, dtype=torch.float64)\n",
      "7800 tensor(13.5789, dtype=torch.float64) tensor(0.1999, dtype=torch.float64)\n",
      "7900 tensor(4.9845, dtype=torch.float64) tensor(0.1759, dtype=torch.float64)\n",
      "8000 tensor(1.8313, dtype=torch.float64) tensor(0.1899, dtype=torch.float64)\n",
      "8100 tensor(0.9026, dtype=torch.float64) tensor(0.2054, dtype=torch.float64)\n",
      "8200 tensor(4.0423, dtype=torch.float64) tensor(0.1773, dtype=torch.float64)\n",
      "8300 tensor(1.4859, dtype=torch.float64) tensor(0.1930, dtype=torch.float64)\n",
      "8400 tensor(91.7558, dtype=torch.float64) tensor(0.2927, dtype=torch.float64)\n",
      "8500 tensor(33.6720, dtype=torch.float64) tensor(0.2335, dtype=torch.float64)\n",
      "8600 tensor(12.3577, dtype=torch.float64) tensor(0.1971, dtype=torch.float64)\n",
      "8700 tensor(4.5364, dtype=torch.float64) tensor(0.1752, dtype=torch.float64)\n",
      "8800 tensor(1.6670, dtype=torch.float64) tensor(0.1913, dtype=torch.float64)\n",
      "8900 tensor(0.9846, dtype=torch.float64) tensor(0.2080, dtype=torch.float64)\n",
      "9000 tensor(1.5064, dtype=torch.float64) tensor(0.2006, dtype=torch.float64)\n",
      "9100 tensor(20.9937, dtype=torch.float64) tensor(0.3079, dtype=torch.float64)\n",
      "9200 tensor(7.7053, dtype=torch.float64) tensor(0.2576, dtype=torch.float64)\n",
      "9300 tensor(2.8294, dtype=torch.float64) tensor(0.2248, dtype=torch.float64)\n",
      "9400 tensor(1.0435, dtype=torch.float64) tensor(0.2022, dtype=torch.float64)\n",
      "9500 tensor(0.9018, dtype=torch.float64) tensor(0.2108, dtype=torch.float64)\n",
      "9600 tensor(0.8954, dtype=torch.float64) tensor(0.2115, dtype=torch.float64)\n",
      "9700 tensor(1.1465, dtype=torch.float64) tensor(0.2042, dtype=torch.float64)\n",
      "9800 tensor(1.7329, dtype=torch.float64) tensor(0.2115, dtype=torch.float64)\n",
      "9900 tensor(0.9927, dtype=torch.float64) tensor(0.2072, dtype=torch.float64)\n",
      "10000 tensor(0.9228, dtype=torch.float64) tensor(0.2152, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "## Line 1\n",
    "N = 9\n",
    "a = torch.zeros(N,requires_grad=True, dtype=torch.float64)\n",
    "loss = Mse(Fmodel(a, xvals), lpsa)\n",
    "loss.backward()\n",
    "with torch.no_grad():\n",
    "    a -= a.grad * 1e-5\n",
    "    a.grad.zero_()\n",
    "nu = 0\n",
    "ncount = 0\n",
    "NMAX = 10000\n",
    "\n",
    "## Line 2\n",
    "while True:\n",
    "    ## Line 3\n",
    "    R = Mse(Fmodel(a, xvals), lpsa);R.backward(); g = -a.grad; a.grad.zero_() #(24)\n",
    "    P = abs(a).pow(1).sum(); P.backward(); p = abs(a.grad); a.grad.zero_() #(25)\n",
    "    l = g/p #(26)\n",
    "    ## Line 4\n",
    "    # use element wise multiplication and less than 0 predicate\n",
    "    # to find elements with corresponding opposite sign\n",
    "    S = l*a < 0\n",
    "    ## Line 5\n",
    "    # need to maintain idx location, i.e. need to keep tensor shape the same throughout\n",
    "    # torch.max() finds max element and respective index\n",
    "    if S.sum() > 0: # check for elements in set\n",
    "        # non empty case (order different than stated algorithm)\n",
    "        # S.double() for matching element types needed by PyTorch\n",
    "        # element wise mult to zero out elements not meeting predicate condition\n",
    "        # done this way to make sure idx refer to corresponding element\n",
    "        val,idx = torch.max(S.double() * l.abs(), 0)\n",
    "    else:\n",
    "        # empty case\n",
    "        val,idx = torch.max(l.abs(), 0)\n",
    "    ## Line 6\n",
    "    with torch.no_grad():\n",
    "        # recall that no gradients should be calculated here while a is being updated\n",
    "        del_nu = 0.01 * (R / g[idx]).abs()\n",
    "        a[idx] += del_nu * torch.sign(l[idx])\n",
    "        # should be 'close' to 0.01\n",
    "        # R_post = mse(Fmodel(a, xvals), lpsa); print(1-(R_post/R))\n",
    "        a.grad.zero_()\n",
    "    ## Line 7\n",
    "    nu += del_nu\n",
    "    ## Line 8\n",
    "    #print(l.sum())\n",
    "    if (ncount % 100 == 0): \n",
    "        print(ncount,R.data,P.data)\n",
    "    if (ncount > NMAX) or (l.abs().sum() <= 0):\n",
    "        break\n",
    "    ncount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.9568e-05, 1.6942e-01, 1.8412e-04, 3.1989e-02, 1.0935e-05, 1.6087e-05,\n",
       "        8.6427e-06, 3.4080e-04, 1.2265e-02], dtype=torch.float64,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0041, 0.1258, 0.0433, 0.0262, 0.0232, 0.0315, 0.0709, 0.0301, 0.0105],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -0.2527,   0.6937,  -0.6433, -18.8883,   0.2337,   0.2396,   0.7350,\n",
       "         -1.6861,  -6.8309], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some families of $P(a)$ from the paper and their gradient. Here k will be a proxy for $\\gamma$ (Power family) or $\\beta$ (Generalized elastic net and Subset selection), and w are weights that serve the role of the parameters $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Power(w,k):\n",
    "    return w.abs().pow(k)\n",
    "\n",
    "def Power_grad(w,k):\n",
    "    return (k*w.abs()*w.abs().pow(k-1))/w\n",
    "\n",
    "def ENet(w,k):\n",
    "    return ((k-1)*(w.pow(2)))/2+((2-k)*w.abs())\n",
    "\n",
    "def ENet_grad(w,k):\n",
    "    return ((2-k)*w.abs()+(k-1)*w.pow(2))/w\n",
    "\n",
    "def SubsetSel(w,k):\n",
    "    return torch.log((1-k)*w.abs()+k)\n",
    "\n",
    "def SubsetSel_grad(w,k):\n",
    "    return ((k-1)*w.abs())/((k-1)*w*w.abs()-k*w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably a good idea to check PyTorch's autograd functionality againt those defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.9568e-05, 1.6942e-01, 1.8412e-04, 3.1989e-02, 1.0935e-05, 1.6087e-05,\n",
      "        8.6427e-06, 3.4080e-04, 1.2265e-02], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "tensor([-0.3333, -0.3533, -0.3334, -0.3369, -0.3333, -0.3333, -0.3333, -0.3334,\n",
      "        -0.3347], dtype=torch.float64)\n",
      "tensor([-0.3333, -0.3533, -0.3334, -0.3369, -0.3333, -0.3333, -0.3333, -0.3334,\n",
      "        -0.3347], dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(a); a.grad.zero_()\n",
    "tr = SubsetSel(a,1.5).sum()\n",
    "tr.backward()\n",
    "print(a.grad)\n",
    "a.grad.zero_()\n",
    "print(SubsetSel_grad(a,1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.9568e-05, 1.6942e-01, 1.8412e-04, 3.1989e-02, 1.0935e-05, 1.6087e-05,\n",
      "        8.6427e-06, 3.4080e-04, 1.2265e-02], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "tensor([9.9135e-05, 3.3884e-01, 3.6824e-04, 6.3979e-02, 2.1870e-05, 3.2174e-05,\n",
      "        1.7285e-05, 6.8160e-04, 2.4531e-02], dtype=torch.float64)\n",
      "tensor([9.9135e-05, 3.3884e-01, 3.6824e-04, 6.3979e-02, 2.1870e-05, 3.2174e-05,\n",
      "        1.7285e-05, 6.8160e-04, 2.4531e-02], dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(a);a.grad.zero_()\n",
    "tr = Power(a,2).sum()\n",
    "tr.backward()\n",
    "print(a.grad)\n",
    "a.grad.zero_()\n",
    "print(Power_grad(a,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.9568e-05, 1.6942e-01, 1.8412e-04, 3.1989e-02, 1.0935e-05, 1.6087e-05,\n",
      "        8.6427e-06, 3.4080e-04, 1.2265e-02], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "tensor([0.5000, 0.5847, 0.5001, 0.5160, 0.5000, 0.5000, 0.5000, 0.5002, 0.5061],\n",
      "       dtype=torch.float64)\n",
      "tensor([0.5000, 0.5847, 0.5001, 0.5160, 0.5000, 0.5000, 0.5000, 0.5002, 0.5061],\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(a);a.grad.zero_()\n",
    "tr = ENet(a,1.5).sum()\n",
    "tr.backward()\n",
    "print(a.grad)\n",
    "a.grad.zero_()\n",
    "print(ENet_grad(a,1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is now ready for experimentation, extension, and probabaly correction - particularly with regrads to interpretation of \n",
    "$$ \\left[ \\frac{\\partial P(a)}{\\partial \\left| a_{j} \\right|} \\right]_{a=\\hat{a}(\\nu)}  \\hspace{1in} (25) $$\n",
    "The domain of $ \\hat{R}(a) $, $\\{a_{j}\\}_{1}^{n}$, is not $\\mathbb{R}^{n}_{+}$ but $\\mathbb{R}^{n}$. Thus, $\\{a_{j}\\}_{1}^{n}$ can freely approach and cross $0$ as part of the evolution of $ \\hat{R}(a) $. $P(a)$ should be considered $ f:\\mathbb{R}^{n} \\to \\mathbb{R}_{+} $. Equation (25) is expressed as partial with respect to $\\left| a_{j} \\right|$ which seems to imply\n",
    "$$ \\left[ \\frac{\\partial P(a)}{ \\partial a_{j}} \n",
    "\\frac{ \\partial a_{j}}{\\partial \\left| a_{j} \\right|}\n",
    "\\right]_{a=\\hat{a}(\\nu)}  \\hspace{1in} (25') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to think about this more. $\n",
    "\\frac{ \\partial a_{j}}{\\partial \\left| a_{j} \\right|} $ ...\n",
    "Oriented space? Otherwise has little bearing. Re-read Tao's paper - He gave a very clear exposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
